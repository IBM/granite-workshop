{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U48hO1_V_JRG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building an AI-powered multimodal RAG system with Docling and Granite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using IBM Granite vision, text-based embeddings and generative AI models*\n",
    "\n",
    "## Multimodal retrieval-augmented generation\n",
    "\n",
    "[Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) is a technique used with large language models (LLMs) to connect the model with a knowledge base of information outside the data the LLM has been trained on without having to perform [fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning). Traditional RAG is limited to text-based use cases such as text summarization and chatbots.\n",
    "\n",
    "Multimodal RAG can use [multimodal](https://www.ibm.com/think/topics/multimodal-ai) LLMs (MLLM) to process information from multiple types of data to be included as part of the external knowledge base used in RAG. Multimodal data can include text, images, audio, video or other forms. Popular multimodal LLMs include Google’s Gemini, Meta’s Llama 3.2 and OpenAI’s GPT-4 and GPT-4o. \n",
    "\n",
    "For this recipe, you will use an IBM Granite model capable of processing different modalities. You will create an AI system to answer real-time user queries from unstructured data in a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eDALN1A9LF8"
   },
   "source": [
    "## Recipe overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Granite recipe. In this recipe, you'll learn how to harness the power of advanced tools to build an AI-powered multimodal RAG pipeline. This tutorial will guide you through the following processes:\n",
    "\n",
    "- **Document preprocessing:** Learn how to handle documents from various sources, parse and transform them into usable formats and store them in vector databases by using Docling. You will use a Granite MLLM to generate image descriptions of images in the documents.\n",
    "- **RAG:** Understand how to connect LLMs such as Granite with external knowledge bases to enhance query responses and generate valuable insights.\n",
    "- **LangChain for workflow integration:** Discover how to use LangChain to streamline and orchestrate document processing and retrieval workflows, enabling seamless interaction between different components of the system.\n",
    "\n",
    "This recipe uses three cutting-edge technologies:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
    "2. **[Granite](https://www.ibm.com/granite/docs/models/granite/):** A state-of-the-art LLM that provides robust natural language capabilities and a vision language model that provides image to text generation.\n",
    "3. **[LangChain](https://github.com/langchain-ai/langchain):** A powerful framework used to build applications powered by language models, designed to simplify complex workflows and integrate external tools seamlessly.\n",
    "\n",
    "By the end of this recipe, you will accomplish the following:\n",
    "- Gain proficiency in document preprocessing, chunking and image understanding.\n",
    "- Integrate vector databases to enhance retrieval capabilities.\n",
    "- Use RAG to perform efficient and accurate data retrieval for real-world applications.\n",
    "\n",
    "This recipe is designed for AI developers, researchers and enthusiasts looking to enhance their knowledge of document management and advanced natural language processing (NLP) techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vooxv7ltEZBf"
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Familiarity with Python programming.\n",
    "- Basic understanding of LLMs, NLP concepts and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN2mK175_JRH",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfg8kTVr_JRH"
   },
   "source": [
    "Ensure you are running Python 3.10, 3.11 or 3.12 in a freshly created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEoM938B_JRH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p_2cX1-_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfMWUUSs_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    pillow \\\n",
    "    langchain_community \\\n",
    "    langchain_huggingface \\\n",
    "    langchain_milvus \\\n",
    "    docling \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gu-Oeay_JRJ"
   },
   "source": [
    "## Step 3: Selecting the AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see some logging information, we can configure INFO log level.\n",
    "\n",
    "NOTE: It is okay to skip running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Granite models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFuZBhG-_JRJ"
   },
   "source": [
    "Specify the embeddings model to use for generating text embedding vectors. Here we will use one of the [Granite Embeddings models](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)\n",
    "\n",
    "To use a different embeddings model, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvztNZly_JRJ"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the MLLM to use for image understanding. We will use the Granite vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_community.llms import Replicate\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "vision_model_path = \"ibm-granite/granite-vision-3.2-2b\"\n",
    "vision_model = Replicate(\n",
    "    model=vision_model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": embeddings_tokenizer.max_len_single_sentence, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
    "    },\n",
    ")\n",
    "vision_processor = AutoProcessor.from_pretrained(vision_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma8eWR10_JRJ"
   },
   "source": [
    "Specify the language model to use for the RAG generation operation. Here we use the Replicate LangChain client to connect to a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ckyj7Zrh_JRK"
   },
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = Replicate(\n",
    "    model=model_path,\n",
    "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviHG3n7_JRK",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 4: Preparing the documents for the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ7Guu7A_JRK"
   },
   "source": [
    "In this example, from a set of source documents, we use [Docling](https://docling-project.github.io/docling/) to convert the documents into text and images. The text is then split into chunks. The images are processed by the MLLM to generate image summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuB8kkzf_JRK"
   },
   "source": [
    "### Use Docling to download the documents and convert to text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se6So2yw_JRK"
   },
   "source": [
    "Docling will download the PDF documents and process them so we can obtain the text and images the documents contain. In the PDF, there are various data types, including text, tables, graphs and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNGz_0gZ_JRK"
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pdf_pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=False,\n",
    "    generate_picture_images=True,\n",
    ")\n",
    "format_options = {\n",
    "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
    "}\n",
    "converter = DocumentConverter(format_options=format_options)\n",
    "\n",
    "sources = [\n",
    "    \"https://midwestfoodbank.org/images/AR_2020_WEB2.pdf\",\n",
    "]\n",
    "conversions = { source: converter.convert(source=source).document for source in sources }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the documents processed, we then further process the text elements in the documents. We chunk them into appropriate sizes for the embeddings model we are using. A list of LangChain documents are created from the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
    "from docling_core.types.doc.document import TableItem\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc_id = 0\n",
    "texts: list[Document] = []\n",
    "for source, docling_document in conversions.items():\n",
    "    for chunk in HybridChunker(tokenizer=embeddings_tokenizer).chunk(docling_document):\n",
    "        items = chunk.meta.doc_items\n",
    "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
    "            continue # we will process tables later\n",
    "        refs = \" \".join(map(lambda item: item.get_ref().cref, items))\n",
    "        print(refs)\n",
    "        text = chunk.text\n",
    "        document = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"doc_id\": (doc_id:=doc_id+1),\n",
    "                \"source\": source,\n",
    "                \"ref\": refs,\n",
    "            },\n",
    "        )\n",
    "        texts.append(document)\n",
    "\n",
    "print(f\"{len(texts)} text document chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we process any tables in the documents. We convert the table data to markdown format for passing into the language model. A list of LangChain documents are created from the table's markdown renderings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "\n",
    "doc_id = len(texts)\n",
    "tables: list[Document] = []\n",
    "for source, docling_document in conversions.items():\n",
    "    for table in docling_document.tables:\n",
    "        if table.label in [DocItemLabel.TABLE]:\n",
    "            ref = table.get_ref().cref\n",
    "            print(ref)\n",
    "            text = table.export_to_markdown(docling_document)\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref\n",
    "                },\n",
    "            )\n",
    "            tables.append(document)\n",
    "\n",
    "\n",
    "print(f\"{len(tables)} table documents created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we process any images in the documents. Here we use the vision language model to understand the content of an image. In this example, we are interested in any textual information in the image. You might want to experiment with different prompt text to see how it might improve the results.\n",
    "\n",
    "NOTE: Processing the images can take a very long time depending upon the number of images and the service running the vision language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
    "    image = PIL.ImageOps.exif_transpose(image) or image\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format)\n",
    "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    uri = f\"data:image/{format};base64,{encoding}\"\n",
    "    return uri\n",
    "\n",
    "# Feel free to experiment with this prompt\n",
    "image_prompt = \"If the image contains text, explain the text in the image.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": image_prompt},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "vision_prompt = vision_processor.apply_chat_template(\n",
    "    conversation=conversation,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "pictures: list[Document] = []\n",
    "doc_id = len(texts) + len(tables)\n",
    "for source, docling_document in conversions.items():\n",
    "    for picture in docling_document.pictures:\n",
    "        ref = picture.get_ref().cref\n",
    "        print(ref)\n",
    "        image = picture.get_image(docling_document)\n",
    "        if image:\n",
    "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"doc_id\": (doc_id:=doc_id+1),\n",
    "                    \"source\": source,\n",
    "                    \"ref\": ref,\n",
    "                },\n",
    "            )\n",
    "            pictures.append(document)\n",
    "\n",
    "print(f\"{len(pictures)} image descriptions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then display the LangChain documents created from the input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htIYVVjHPKSX"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from docling_core.types.doc.document import RefItem\n",
    "from IPython.display import display\n",
    "\n",
    "# Print all created documents\n",
    "for document in itertools.chain(texts, tables):\n",
    "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
    "    print(f\"Source: {document.metadata['source']}\")\n",
    "    print(f\"Content:\\n{document.page_content}\")\n",
    "    print(\"=\" * 80)  # Separator for clarity\n",
    "\n",
    "for document in pictures:\n",
    "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
    "    source = document.metadata['source']\n",
    "    print(f\"Source: {source}\")\n",
    "    print(f\"Content:\\n{document.page_content}\")\n",
    "    docling_document = conversions[source]\n",
    "    ref = document.metadata['ref']\n",
    "    picture = RefItem(cref=ref).resolve(docling_document)\n",
    "    image = picture.get_image(docling_document)\n",
    "    print(\"Image:\")\n",
    "    display(image)\n",
    "    print(\"=\" * 80)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bjz1IR3_JRK",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Populate the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the embedding model, we load the documents from the text chunks and generated image captioning into a vector database. Creating this vector database allows us to easily conduct a semantic similarity search across our documents.\n",
    "\n",
    "NOTE: Population of the vector database can take some time depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the database to use for storing and retrieving embedding vectors.\n",
    "\n",
    "To connect to a vector database other than Milvus, replace this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore_\", suffix=\".db\", delete=False).name\n",
    "print(f\"The vector database will be saved to {db_file}\")\n",
    "\n",
    "vector_db: VectorStore = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": db_file},\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,\n",
    "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add all the LangChain documents for the text, tables and image descriptions to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSbVb6R4_JRK"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "documents = list(itertools.chain(texts, tables, pictures))\n",
    "ids = vector_db.add_documents(documents)\n",
    "print(f\"{len(ids)} documents added to the vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq50gMAO_JRK"
   },
   "source": [
    "## Step 5: RAG with Granite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZOYJW0D_JRL"
   },
   "source": [
    "Now that we have successfully converted our documents and vectorized them, we can set up out RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC_KSxFk_JRL"
   },
   "source": [
    "### Retrieve relevant chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf8T8eZk_JRL"
   },
   "source": [
    "Here we test the vector database by searching for chunks with relevant information to our query in the vector space. We display the documents associated with the retrieved image description.\n",
    "\n",
    "Feel free to try different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMtTSHhQ_JRL"
   },
   "outputs": [],
   "source": [
    "query = \"How much was spent on food distribution relative to the amount of food distributed?\"\n",
    "for doc in vector_db.as_retriever().invoke(query):\n",
    "    print(doc)\n",
    "    print(\"=\" * 80)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viFYsnxTS2OC"
   },
   "source": [
    "The returned document should be responsive to the query. Let's go ahead and construct our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxVuFY_A_JRL"
   },
   "source": [
    "### Create the RAG pipeline for Granite\n",
    "\n",
    "First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.\n",
    "\n",
    "`{context}` will hold the retrieved chunks, as shown in the previous search, and feeds this to the model as document context for answering our question.\n",
    "\n",
    "Next, we construct the RAG pipeline by using the Granite prompt templates previously created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB-CPPTo_JRL"
   },
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import escape_f_string\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create a Granite prompt for question-answering with the retrieved context\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"doc_id\": \"0\",\n",
    "        \"text\": \"{context}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# The Granite prompt can contain JSON strings, so we must escape them\n",
    "prompt_template = PromptTemplate.from_template(template=escape_f_string(prompt, \"input\", \"context\"))\n",
    "\n",
    "# Create a Granite document prompt template to wrap each retrieved document\n",
    "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
    "{page_content}\"\"\")\n",
    "document_separator=\"\"\n",
    "\n",
    "# Assemble the retrieval-augmented generation chain\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_NU_Yhl_JRQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXQzDqAB_JRQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pipeline uses the query to locate documents from the vector database and use them as context for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hdo9PXsU_JRQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_uyUdCu_JRQ"
   },
   "source": [
    "Awesome! We have created an AI application that can successfully leverage knowledge from the source documents' text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTaX1-SPT6rU"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore advanced RAG workflows for other industries.\n",
    "- Experiment with other document types and larger datasets.\n",
    "- Optimize prompt engineering for better Granite responses.\n",
    "\n",
    "Thank you for using this recipe!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
