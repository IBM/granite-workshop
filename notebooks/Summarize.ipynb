{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates an application of long document summarization techniques to a work of literature using Granite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are running python 3.10, 3.11, or 3.12 in a freshly-created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwS1CzAbaFzq"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Granite utils provides some helpful functions for recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    langchain_community \\\n",
    "    transformers \\\n",
    "    langchain_huggingface \\\n",
    "    langchain_ollama \\\n",
    "    replicate \\\n",
    "    docling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving the Granite AI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook requires IBM Granite models to be served by an AI model runtime so that the models can be invoked or called. This notebook can use a locally accessible [Ollama](https://github.com/ollama/ollama) server to serve the models, or the [Replicate](https://replicate.com) cloud service.\n",
    "\n",
    "During the pre-work, you may have either started a local Ollama server on your computer, or setup Replicate access and obtained an [API token](https://replicate.com/account/api-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Select your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Granite model to use. Here we use a Langchain client to connect to the model. If there is a locally accessible Ollama server, we use an Ollama client to access the model. Otherwise, we use a Replicate client to access the model.\n",
    "\n",
    "When using Replicate, if the `REPLICATE_API_TOKEN` environment variable is not set, or a `REPLICATE_API_TOKEN` Colab secret is not set, then the notebook will ask for your [Replicate API token](https://replicate.com/account/api-tokens) in a dialog box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "try: # Look for a locally accessible Ollama server for the model\n",
    "    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n",
    "    model = OllamaLLM(\n",
    "        model=\"granite3.3:8b\",\n",
    "        num_ctx=65536, # 64K context window\n",
    "    )\n",
    "    model = model.bind(raw=True) # Client side controls prompt\n",
    "except Exception: # Use Replicate for the model\n",
    "    model = Replicate(\n",
    "        model=model_path,\n",
    "        replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 2000, # Set the maximum number of tokens to generate as output.\n",
    "            \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "            \"temperature\": 0.75,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Split a book into chunks\n",
    "\n",
    "The code in the following sections will fetch H.D. Thoreau's \"Walden\" from [Project Gutenberg](https://www.gutenberg.org/) for summarization.\n",
    "\n",
    "We will then chunk the book text so that chunks fit in the context window size of the AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuQmgRJY0n5"
   },
   "source": [
    "### Count the tokens\n",
    "\n",
    "Before sending our book chunks to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the [`granite-3.3`](https://huggingface.co/ibm-granite/granite-3.3-2b-instruct) model, which has a context window of 128K tokens.\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model.\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Summaries\n",
    "\n",
    "Here we use a hierarchical abstractive summarization technique to adapt to the context length of the model. Our approach uses [Docling](https://docling-project.github.io/docling/) to understand the document's structure, chunk the document into text passages, and group the text passages by chapter which we can then summarize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Iterator, Callable\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "from docling_core.transforms.chunker.base import BaseChunk\n",
    "\n",
    "def chunk_document(source: str, *, dropwhile: Callable[[BaseChunk], bool] = lambda c: False, takewhile: Callable[[BaseChunk], bool] = lambda c: True) -> Iterator[BaseChunk]:\n",
    "    \"\"\"Read the document and perform a hierarchical chunking\"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    chunks = HierarchicalChunker().chunk(converter.convert(source=source).document)\n",
    "    return itertools.takewhile(takewhile, itertools.dropwhile(dropwhile, chunks))\n",
    "\n",
    "def merge_chunks(chunks: Iterator[BaseChunk], *, headings: Callable[[BaseChunk], list[str]] = lambda c: c.meta.headings) -> Iterator[dict[str, str]]:\n",
    "    \"\"\"Merge chunks having the same headings\"\"\"\n",
    "    prior_headings: list[str] | None = None\n",
    "    document: dict[str, str] = {}\n",
    "    doc_id = 0\n",
    "    for chunk in chunks:\n",
    "        text = chunk.text.replace('\\r\\n', '\\n')\n",
    "        current_headings = headings(chunk)\n",
    "        if prior_headings != current_headings:\n",
    "            if document:\n",
    "                yield document\n",
    "            prior_headings = current_headings\n",
    "            document = {\n",
    "                'doc_id': str(doc_id := doc_id + 1),\n",
    "                'title': \" - \".join(current_headings),\n",
    "                'text': text\n",
    "            }\n",
    "        else:\n",
    "            document['text'] += f\"\\n\\n{text}\"\n",
    "    if document:\n",
    "        yield document\n",
    "\n",
    "def chunk_dropwhile(chunk: BaseChunk) -> bool:\n",
    "    \"\"\"Ignore front matter prior to the book start\"\"\"\n",
    "    return \"WALDEN\" not in chunk.meta.headings\n",
    "\n",
    "def chunk_takewhile(chunk: BaseChunk) -> bool:\n",
    "    \"\"\"Ignore remaining chunks once we see this heading\"\"\"\n",
    "    return \"ON THE DUTY OF CIVIL DISOBEDIENCE\" not in chunk.meta.headings\n",
    "\n",
    "def chunk_headings(chunk: BaseChunk) -> list[str]:\n",
    "    \"\"\"Use the h1 and h2 (chapter) headings\"\"\"\n",
    "    return chunk.meta.headings[:2]\n",
    "\n",
    "documents: list[dict[str, str]] = list(merge_chunks(\n",
    "    chunk_document(\n",
    "        \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\",\n",
    "        dropwhile=chunk_dropwhile,\n",
    "        takewhile=chunk_takewhile,\n",
    "    ),\n",
    "    headings=chunk_headings,\n",
    "))\n",
    "\n",
    "print(f\"{len(documents)} documents created\")\n",
    "print(f\"Max document size: {max(len(tokenizer.tokenize(document['text'])) for document in documents)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the chunks\n",
    "\n",
    "Here we define a method to generate a response using a list of documents and a user prompt about those documents. \n",
    "\n",
    "We create the prompt according to the [Granite Prompting Guide](https://www.ibm.com/granite/docs/models/granite/#basic-chat-template-example) and provide the documents using the `documents` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(user_prompt: str, documents: list[dict[str, str]]):\n",
    "    \"\"\"Use the chat template to format the prompt\"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }],\n",
    "        documents=documents, # This uses the documents support in the Granite chat template\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Input size: {len(tokenizer.tokenize(prompt))} tokens\")\n",
    "    output = model.invoke(prompt)\n",
    "    print(f\"Output size: {len(tokenizer.tokenize(output))} tokens\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chapter, we create a separate summary. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_env_var('GRANITE_TESTING', 'false').lower() == 'true':\n",
    "    documents = documents[:5] # shorten testing work\n",
    "\n",
    "user_prompt = \"\"\"\\\n",
    "Using only the the book chapter document, compose a summary of the book chapter.\n",
    "Your response should only include the summary. Do not provide any further explanation.\"\"\"\n",
    "\n",
    "summaries: list[dict[str, str]] = []\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"============================= {document['title']} ({i+1}/{len(documents)}) =============================\")\n",
    "    output = generate(user_prompt, [document])\n",
    "    summaries.append({\n",
    "        'doc_id': document['doc_id'],\n",
    "        'title': document['title'],\n",
    "        'text': output,\n",
    "    })\n",
    "\n",
    "print(\"Summary count: \" + str(len(summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Final Summary\n",
    "\n",
    "Now we need to summarize the chapter summaries. We prompt the model to create a unified summary of the chapter summaries we previously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "user_prompt = \"\"\"\\\n",
    "Using only the book chapter summary documents, compose a single, unified summary of the book.\n",
    "Your response should only include the unified summary. Do not provide any further explanation.\"\"\"\n",
    "\n",
    "output = generate(user_prompt, summaries)\n",
    "print(wrap_text(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now summarized a document larger than the AI model's context window length by breaking the document down into smaller pieces to summarize and then summarizing those summaries."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
